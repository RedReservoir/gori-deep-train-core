import numpy
import torch



class StepWiseLossRegister():
    """
    Keeps track and manages loss-related values across multiple steps and epochs.

    In-memory epoch-wise data stored:
        - Total loss value
        - Total loss items
        - Total NaN steps

    In-memory step-wise data stored:
        - Total loss value
        - Total loss items
        - NaN detected flag

    In a distributed setting, accumulation of step data occurs locally in every subprocess.
    Synchronization must be done via the `synchronize` method.

    Only step data buffers from the current epoch are stored in memory, which are reset at the
    beginning of every epoch, but it is possible to save them into files.
    """

    def __init__(
        self
    ):

        self._epoch_total_loss_list = []
        self._epoch_total_items_list = []
        self._epoch_total_nan_steps_list = []


    def initialize_step_data(
        self,
        epoch_num_steps
    ):
        """
        Initializes step and epoch data buffers.
        Must be called at the beginning of every epoch.
        
        :param epoch_num_steps: int
            Number of steps expected in this epoch.
        """

        self._curr_epoch_step_total_loss_arr = numpy.empty(shape=(epoch_num_steps), dtype=float)
        self._curr_epoch_step_total_items_arr = numpy.empty(shape=(epoch_num_steps), dtype=int)
        self._curr_epoch_step_nan_flag_arr = numpy.empty(shape=(epoch_num_steps), dtype=bool)

        self._curr_step_total_loss = 0.0
        self._curr_step_total_items = 0
        self._curr_step_nan_flag = False

        self._sync_step_num = 0
        self._curr_step_num = 0


    def accumulate_batch(
        self,
        batch_loss,
        batch_items
    ):
        """
        Accumulates one batch worth of loss into the step data buffers.
        Must be called after every ModuleTransform forward pass.

        :param batch_loss: float
            Total loss generated by the batch.
        :param batch_items: int
            Number of training examples in the batch. 
        """

        if not self._curr_step_nan_flag:
            self._curr_step_total_loss += batch_loss
            self._curr_step_total_items += batch_items


    def mark_nan_step(
        self
    ):
        """
        Marks the current step as a NaN step.
        Must be called after every step, if NaN losses were detected.
        """

        self._curr_step_total_loss = 0.0
        self._curr_step_total_items = 0
        self._curr_step_nan_flag = True


    def store_curr_step_data(
        self
    ):
        """
        Stores the current step data from the step data buffers into the epoch data buffers.
        Must be called at the end of every step.
        """
        
        self._curr_epoch_step_total_loss_arr[self._curr_step_num] = self._curr_step_total_loss
        self._curr_epoch_step_total_items_arr[self._curr_step_num] = self._curr_step_total_items
        self._curr_epoch_step_nan_flag_arr[self._curr_step_num] = self._curr_step_nan_flag

        self._curr_step_total_loss = 0.0
        self._curr_step_total_items = 0
        self._curr_step_nan_flag = False

        self._curr_step_num += 1


    def synchronize_epoch_data(
        self
    ):
        """
        Synchronizes epoch data buffers among all subprocesses.
        Must be called at the end of every epoch, immediately before `store_curr_epoch_data`.
        Can also be called at any point of the epoch if necessary, and sequential calls are
        idempotent, but calling this method often can incur heavy slowdowns.
        """
        
        if self._sync_step_num == self._curr_step_num: return

        rank = torch.distributed.get_rank()
        device = torch.device(rank)

        with torch.no_grad():

            sync_curr_step_total_loss_ten = torch.FloatTensor(self._curr_epoch_step_total_loss_arr[self._sync_step_num:self._curr_step_num]).to(device)
            sync_curr_step_total_items_ten = torch.FloatTensor(self._curr_epoch_step_total_items_arr[self._sync_step_num:self._curr_step_num]).to(device)
            sync_curr_step_nan_flag_ten = torch.FloatTensor(self._curr_epoch_step_nan_flag_arr[self._sync_step_num:self._curr_step_num]).to(device)

            torch.distributed.all_reduce(sync_curr_step_total_loss_ten, torch.distributed.ReduceOp.SUM)
            torch.distributed.all_reduce(sync_curr_step_total_items_ten, torch.distributed.ReduceOp.SUM)
            torch.distributed.all_reduce(sync_curr_step_nan_flag_ten, torch.distributed.ReduceOp.MAX)

            self._curr_epoch_step_total_loss_arr[self._sync_step_num:self._curr_step_num] = sync_curr_step_total_loss_ten.cpu().numpy().astype(float)
            self._curr_epoch_step_total_items_arr[self._sync_step_num:self._curr_step_num] = sync_curr_step_total_items_ten.cpu().numpy().astype(int)
            self._curr_epoch_step_nan_flag_arr[self._sync_step_num:self._curr_step_num] = sync_curr_step_nan_flag_ten.cpu().numpy().astype(bool)

        self._sync_step_num = self._curr_step_num


    def store_curr_epoch_data(
        self
    ):
        """
        Reduces epoch data buffers and stores them into the epoch data lists.
        Must be called at the end of every epoch, immediately after `synchronize_epoch_data`.
        """

        self._epoch_total_loss_list.append(numpy.sum(self._curr_epoch_step_total_loss_arr))
        self._epoch_total_items_list.append(numpy.sum(self._curr_epoch_step_total_items_arr))
        self._epoch_total_nan_steps_list.append(numpy.sum(self._curr_epoch_step_nan_flag_arr))
       

    def save_step_data(
        self,
        filename
    ):
        """
        Saves the step data arrays into a file.
        Should be called at the end of every epoch.

        :param filename: str
            Filename where to save step data arrays into.
            Must have `.npz` extension.
        """

        numpy.savez(
            filename,
            step_total_loss_arr=self._curr_epoch_step_total_loss_arr,
            step_total_items_arr=self._curr_epoch_step_total_items_arr,
            step_nan_flag_arr=self._curr_epoch_step_nan_flag_arr
        )


    def save_epoch_data(
        self,
        filename
    ):
        """
        Saves the epoch data lists into a file.

        :param filename: str
            Filename where to save epoch data lists into.
            Must have `.npz` extension.
        """

        numpy.savez(
            filename,
            epoch_total_loss_arr=numpy.asarray(self._epoch_total_loss_list, dtype=float),
            epoch_total_items_arr=numpy.asarray(self._epoch_total_items_list, dtype=int),
            epoch_total_nan_steps_list=numpy.asarray(self._epoch_total_nan_steps_list, dtype=bool)
        )        


    def load_epoch_data(
        self,
        filename
    ):
        """
        Loads epoch data lists from a file.

        :param filename: str
            Filename where to load epoch data lists from.
        """

        epoch_data = numpy.load(filename)
        self._epoch_total_loss_list = epoch_data["epoch_total_loss_arr"].tolist()
        self._epoch_total_items_list = epoch_data["epoch_total_items_arr"].tolist()
        self._epoch_total_nan_steps_list = epoch_data["epoch_total_nan_steps_list"].tolist()


    ########
    # ACCESSING
    ########


    @property
    def curr_epoch_step_total_loss_arr(self):
        return self._curr_epoch_step_total_loss_arr

    @property
    def curr_epoch_step_total_items_arr(self):
        return self._curr_epoch_step_total_items_arr

    @property
    def curr_epoch_step_nan_flag_arr(self):
        return self._curr_epoch_step_nan_flag_arr
    
    @property
    def epoch_total_loss_list(self):
        return self._epoch_total_loss_list

    @property
    def epoch_total_items_list(self):
        return self._epoch_total_items_list

    @property
    def epoch_total_nan_steps_list(self):
        return self._epoch_total_nan_steps_list
    