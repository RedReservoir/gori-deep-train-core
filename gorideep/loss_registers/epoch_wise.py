import numpy
import torch



class EpochWiseLossRegister():
    """
    Keeps track and manages loss-related values across multiple epochs.

    In-memory epoch-wise data stored:
        - Total loss value
        - Total loss items
        - Total NaN batches

    In a distributed setting, accumulation of epoch data occurs locally in every subprocess.
    Synchronization must be done via the `synchronize` method.
    """

    def __init__(
        self
    ):

        self._epoch_total_loss_list = []
        self._epoch_total_items_list = []
        self._epoch_total_nan_batches_list = []


    def initialize_epoch_data(
        self
    ):
        """
        Initializes epoch data buffers.
        Must be called at the beginning of every epoch.
        """

        self._sync_curr_epoch_total_loss = 0.0
        self._sync_curr_epoch_total_items = 0
        self._sync_curr_epoch_total_nan_batches = 0

        self._sync_required = False

        self._curr_epoch_total_loss = 0.0
        self._curr_epoch_total_items = 0
        self._curr_epoch_total_nan_batches = 0


    def accumulate_batch(
        self,
        batch_loss,
        batch_items
    ):
        """
        Accumulates one batch worth of loss into the epoch data buffers.
        Must be called after every ModuleTransform forward pass.

        :param batch_loss: float
            Total loss generated by the batch.
        :param batch_items: int
            Number of training examples in the batch. 
        """

        self._curr_epoch_total_loss += batch_loss
        self._curr_epoch_total_items += batch_items

        self._sync_required = True


    def accumulate_nan_batch(
        self
    ):
        """
        Accumulates one NaN batch worth of loss into the epoch data buffers.
        Must be called after every ModuleTransform forward pass, if the loss is NaN.
        """

        self._curr_epoch_total_nan_batches += 1

        self._sync_required = True


    def synchronize_epoch_data(
        self
    ):
        """
        Synchronizes epoch data buffers among all subprocesses.
        Must be called at the end of every epoch, immediately before `store_curr_epoch_data`.
        Can also be called at any point of the epoch if necessary, and sequential calls are
        idempotent, but calling this method often can incur heavy slowdowns.
        """

        if not self._sync_required: return

        rank = torch.distributed.get_rank()
        device = torch.device(rank)

        with torch.no_grad():

            sync_curr_epoch_total_loss_ten = torch.FloatTensor([self._curr_epoch_total_loss]).to(device)
            sync_curr_epoch_total_items_ten = torch.FloatTensor([self._curr_epoch_total_items]).to(device)
            sync_curr_epoch_total_nan_batches_ten = torch.FloatTensor([self._curr_epoch_total_nan_batches]).to(device)

            torch.distributed.all_reduce(sync_curr_epoch_total_loss_ten, torch.distributed.ReduceOp.SUM)
            torch.distributed.all_reduce(sync_curr_epoch_total_items_ten, torch.distributed.ReduceOp.SUM)
            torch.distributed.all_reduce(sync_curr_epoch_total_nan_batches_ten, torch.distributed.ReduceOp.SUM)

            self._sync_curr_epoch_total_loss += float(sync_curr_epoch_total_loss_ten.cpu().item())
            self._sync_curr_epoch_total_items += int(sync_curr_epoch_total_items_ten.cpu().item())
            self._sync_curr_epoch_total_nan_batches += int(sync_curr_epoch_total_nan_batches_ten.cpu().item())

        self._sync_required = False

        self._curr_epoch_total_loss = 0.0
        self._curr_epoch_total_items = 0
        self._curr_epoch_total_nan_batches = 0


    def store_curr_epoch_data(
        self
    ):
        """
        Reduces epoch data buffers and stores them into the epoch data lists.
        Must be called at the end of every epoch, immediately after `synchronize_epoch_data`.
        """

        self._epoch_total_loss_list.append(self._sync_curr_epoch_total_loss)
        self._epoch_total_items_list.append(self._sync_curr_epoch_total_items)
        self._epoch_total_nan_batches_list.append(self._sync_curr_epoch_total_nan_batches)
       

    def save_epoch_data(
        self,
        filename
    ):
        """
        Saves the epoch data lists into a file.

        :param filename: str
            Filename where to save epoch data lists into.
            Must have `.npz` extension.
        """

        numpy.savez(
            filename,
            epoch_total_loss_arr=numpy.asarray(self._epoch_total_loss_list, dtype=float),
            epoch_total_items_arr=numpy.asarray(self._epoch_total_items_list, dtype=numpy.uint32),
            epoch_total_nan_batches_arr=numpy.asarray(self._epoch_total_nan_batches_list, dtype=numpy.uint32)
        )        


    def load_epoch_data(
        self,
        filename
    ):
        """
        Loads epoch data lists from a file.

        :param filename: str
            Filename where to load epoch data lists from.
        """

        epoch_data = numpy.load(filename)
        self._epoch_total_loss_list = epoch_data["epoch_total_loss_arr"].tolist()
        self._epoch_total_items_list = epoch_data["epoch_total_items_arr"].tolist()
        self._epoch_total_nan_batches_list = epoch_data["epoch_total_nan_batches_arr"].tolist()


    ########
    # ACCESSING
    ########

    
    @property
    def epoch_total_loss_list(self):
        return self._epoch_total_loss_list

    @property
    def epoch_total_items_list(self):
        return self._epoch_total_items_list

    @property
    def epoch_total_nan_batches_list(self):
        return self._epoch_total_nan_batches_list
    